
<!doctype html>
<html lang="en">	
<head>
<!-- taken from tutorial http://htmlcheats.com/reveal-js/reveal-js-tutorial-reveal-js-for-beginners/ -->
    <meta charset="utf-8">
    <title>Big Data with Hadoop/Spark</title>
    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">    

    <!-- Syntax Highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!--Add support for earlier versions of Internet Explorer -->
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>
<body>
    <!-- Wrap the entire slide show in a div using the "reveal" class. -->
    <div class="reveal">
    <div id="arc-logo" style="background: url(images/arc-logo.png);
                        background-size: 516px 74px;
                        position: absolute;
                        bottom: 30px;
                        left: 50px;
                        width: 516px;
                        height: 74px;"></div>
        <!-- Wrap all slides in a single "slides" class -->
        <div class="slides">
            <section id="title">
                    <h1>Big Data with Hadoop and Spark</h1>
                    <br />
                    <p>Brock Palen</p>
                    <p>brockp@umich.edu</p>
                    <br />
                    <p>Alec Ten Harmsel</p>
                    <p>talec@umich.edu</p>
            </section>
            <section id="what-is-hadoop">
                     <h2>What is Hadoop?</h2>
                     <p>The Apache Hadoop software library is a <strong class="fragment highlight-green">framework</strong> that allows for the <strong class="fragment highlight-green">distributed processing</strong> of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.</p>
                     <br />
                     <br />
                     <br />
                     <h3>What is Fladoop?</h3>
                     <p>Fladoop is the name of the Flux Hadoop cluster, just as Flux is the name of the general HPC cluster.</p>
            </section>
            <section id="links">
                     <h1>Links / Guides</h1>
                     <a href="http://caen.github.io/hadoop">ARC Fladoop Documentation</a><br />
                     <a href="http://fluxhpc.blogspot.com/2014/09/arc-fladoop-data-platform-hadoop.html">Hardware - Fladoop Architecture</a><br />
                     <a href="https://www.udacity.com/course/ud617">Free UDACITY Hadoop/MapReduce Course Materials</a><br />
                     <iframe width="420" height="315" src="http://www.youtube.com/embed/bcjSe0xCHbE" frameborder="0" allowfullscreen></iframe>
            </section>
            <section id="avoid-map-reduce">
                     <h2>Avoid MapReduce</h2>
                        <ul>
                        
                        	<li>Lowest level way to control Hadoop</li>
                        	<li>Most difficult to work with</li>
                        	<li>Java - Take it as you will</li>
                        
                        </ul>
            </section>
            <aside class="notes" data-markdown>
              <p>
	      - Don't focus on MapReduce. 
              - It is difficult and requires building a pair of Java classes.  Most users with big data will be better served by using applications on top of Hadoop. Think of this as writing your own program, vs. using an exsiting one (C-Code vs. R).
              </p>
            </aside>
            <section id="alternatives">
                     <h2>Alternatives to MapReduce</h2>
                        <ul>
                        	<li><a href="http://hive.apache.org/">HIVE - Data Warehouse SQL Like</a></li>
                        	<li><a href="http://pig.apache.org/">Pig - Platform for analyzing large data sets</a></li>
                        	<li><a href="http://spark.apache.org/">Spark - A fast and general compute engine for Hadoop data, Graph Data, and Machine Learning</a></li>
                        	<li><a href="http://mahout.apache.org/">Mahout - A Scalable machine learning and data mining library</a></li>
                        </ul>
			<p>More on these later</p>

            </section>
            <section id="hdfs">
                     <h2>The Hadoop File System (HDFS)</h2>
                     <!-- TODO Fix this image - has fladdop instead of fladoop -->
                     <img src="images/fladoop20.jpg">
            </section>
            <section id="hdfs-about">
                    <h2>About HDFS</h2>
               <ul>
               	<li>Distributed and fault tolerant</li>
               	<li>Canâ€™t use typical tools(cp, mv, mkdir)</li>
                <li>Hadoop has HDFS-specific implementations of common tools</li>
               	<li>Looks like POSIX filesystems</li>
               </ul>
            </section>
            <section id="dfs">
                     <h2>Common HDFS Commands</h2>
                        <pre><code data-trim>
# List files HDFS home directory
$ hdfs dfs -ls
$ hdfs dfs -ls /user/brockp

# Make a directory
$ hdfs dfs -mkdir directory

# Get full list of HDFS options
$ hdfs dfs -help

# Get help with single HDFS option, leave off the -
$ hdfs dfs -help ls
                        </code></pre>
            </section>
            <section id="hdfs-practice1">
	        <section id="hdfs-practice1-q">
                        <h2>HDFS Practice</h2>
                        <br />
                        <p>Create a directory in HDFS called <code>arcdata</code> and check its contents.</p>
                </section>
                <section id="hdfs-practice1-ans">
                    <pre><code>
$ hdfs dfs -mkdir arcdata
$ hdfs dfs -ls arcdata
                    </code></pre>
                </section>
            </section>
            <section id="hdfs-get-put">
		  <h2>Moving to and from HDFS</h2>
		  <h3>get / put / distcp</h3>
		  <pre><code>
$ hdfs dfs -put localfile hdfsdirectory/
$ hdfs dfs -get hdfsfile  localdirectory/

# distcp is handy for very large files
# but requires a shared filesystem (/scratch)
$ hdfs dfs -distcp file:///scratch/my/files hdfs:///user/my/files
		  </code></pre>
           </section>
           <section id="hadoop-examples">
                   <h2>Hadoop Examples</h2>
                   <ul>
                           <li>NGrams schema: word ,year, occurrences, volumes</li>
                           <li>Task: Find number of words per year that occur in only one volume</li>
                   </ul>
                   <br />
                   <p>Put in the data</p>
                   <pre><code>
$ hdfs dfs -put ngrams.data ngrams.data
                   </code></pre>
           </section>
           <section id="hive">
                   <h2>Hive</h2>
                   <pre><code>
$ hive
$
$ > # Now in interactive Hive console
$ >
$ > CREATE TABLE ngrams (ngram STRING, year INT, matches BIGINT,
$ >     volumes BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
$ >
$ > LOAD DATA INPATH 'ngrams.data' OVERWRITE INTO TABLE ngrams;
$ >
$ > SELECT COUNT(ngram),year FROM ngrams WHERE volumes = 1
$ >     GROUP BY year ORDER BY year;
                   </code></pre>
                   <br />
                   <p>Hive supports most of the SQL standard needed on a daily basis.</p>
           </section>
           <section id="spark">
                   <h2>Spark</h2>
                   <pre><code>
$ spark-shell --master yarn-client
$
# Now in interactive Spark console (Scala)
# The Python API is virtually identical
$ > val ngrams = sc.textFile("ngrams.data")
$ > val columns = ngrams.map(line => line.split("\t"))
$ > val filtered = columns.filter(arr => arr(3).toInt == 1)
$ > val reducable = filtered.map(arr => (arr(1), 1))
$ > val answer = reducable.reduceByKey((a, b) => a + b)
$ > answer.first()
                   </code></pre>
                   <br />
                   <p>Spark supports applying transformations to entire datasets (called Resilient Distributed Datasets). This functional programming model allows complicated transformations to be expressed simply.</p>
           </section>
           <section id="pig">
                   <h2>Pig</h2>
                   <pre><code>
$ pig
$
$ > # Now in interactive Pig console
$ > data = LOAD 'warehouse/ngrams/data' AS (ngram:chararray, year:int,
$ >     matches:long, volumes:long);
$ > filtered = FILTER data BY volumes == 1;
$ > grouped = GROUP filtered BY year;
$ > answer = FOREACH grouped GENERATE group, SUM($1.volumes);
$ > ordered = ORDER answer BY $0 ASC;
$ > DUMP ordered;
                   </code></pre>
                   <br />
                   <p>Pig is similar to but more in-depth than SQL. Any complex transformations that can't be done in SQL probably can be expressed in Pig.</p>
           </section>
           <section id="python">
                   <h2>Hadoop Streaming: Python</h2>
                   <pre><code class="python">
# mapper.py
for line in fileinput.input():
    split = line.split("\t")
    if int(split[3]) == 1:
        print('\t'.join([split[1], '1']))

# reducer.py
year_count = dict()
for line in fileinput.input():
    tmp = line.split('\t')
    if tmp[0] not in year_count.keys():
        year_count[tmp[0]] = 0
    year_count[tmp[0]] = year_count[tmp[0]] + 1
for year in sorted(year_count.keys()):
    print('\t'.join([year, year_count[year]]))
                   </code></pre>
                   <br />
                   <p>Python can be run on the cluster by writing a mapper and reducer that both read from STDIN and write key-value pairs to STDOUT.</p>
           </section>
           <section id="choosing">
                   <h2>Choosing a tool and language</h2>
                   <br />
                   <ul>
                           <li>Working on structured data? Use Hive or Pig.</li>
                           <li>Working on unstructured data? Use Python or Scala.</li>
                           <li>Need extremely high performance? Use Java or Scala.</li>
                   </ul>
           </section>
        </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
 
    <script>
        // Required, even if empty.
        Reveal.initialize({
	                  slideNumber: true,
			  history: true,
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        { src: 'plugin/math/math.js', async: true }
    ]
        });

    </script>
</body>
</html>
